{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60694537",
   "metadata": {},
   "source": [
    "# 문제정의\n",
    "\n",
    "- 실제 개 고양이 이미지를 활용해서 개와 고양이를 분류\n",
    "\n",
    "- 데이터 정리 (훈련데이터와 테스터 폴더로 필요한 이미지 파일을 이동)\n",
    "- ImageDataGenerator를 이용해서 전처리\n",
    "- CNN으로 개고양이 분류 모델을 설계하고 학습\n",
    "\n",
    "- 증강학습 : 이미지데이터 늘리기\n",
    "- 전이학습\n",
    "   - 특성추출\n",
    "   - 미세조정\n",
    "   \n",
    "- 과대적합방지\n",
    "  - Dropout\n",
    "  - BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851a169",
   "metadata": {},
   "source": [
    "## train.zip 파일의 압축을 푼다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cb621ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path, shutil, zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5b888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile(\"./data/train.zip\", \"r\")\n",
    "zip_ref.extractall(\"./data/\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61e0ec",
   "metadata": {},
   "source": [
    "## 훈련데이터 2000개, 테스트데이터 500개씩 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7e0905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./data/cat_dog\"\n",
    "if not os.path.exists(base_dir) :\n",
    "    os.mkdir(base_dir)\n",
    "    \n",
    "train_dir = \"./data/cat_dog/train\"\n",
    "if not os.path.exists(train_dir) :    \n",
    "    os.mkdir(train_dir)  \n",
    "    \n",
    "test_dir = \"./data/cat_dog/test\"\n",
    "if not os.path.exists(test_dir) :    \n",
    "    os.mkdir(test_dir)     \n",
    "    \n",
    "train_dog_dir = \"./data/cat_dog/train/dog\"  \n",
    "if not os.path.exists(train_dog_dir) :    \n",
    "    os.mkdir(train_dog_dir) \n",
    "    \n",
    "train_cat_dir = \"./data/cat_dog/train/cat\"  \n",
    "if not os.path.exists(train_cat_dir) :    \n",
    "    os.mkdir(train_cat_dir)     \n",
    "    \n",
    "test_dog_dir = \"./data/cat_dog/test/dog\"  \n",
    "if not os.path.exists(test_dog_dir) :    \n",
    "    os.mkdir(test_dog_dir) \n",
    "    \n",
    "test_cat_dir = \"./data/cat_dog/test/cat\"  \n",
    "if not os.path.exists(test_cat_dir) :    \n",
    "    os.mkdir(test_cat_dir)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4bea3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir2 = \"./data/train\"\n",
    "\n",
    "# 각 폴더로 이미지들을 복사\n",
    "# 0번부터 1999번의 고양이 이미지를 train/cat 폴더로 이동\n",
    "fnames = [\"cat.{}.jpg\".format(i) for i in range(2000)]\n",
    "for fname in fnames :\n",
    "    src = os.path.join(train_dir2, fname)\n",
    "    dst = os.path.join(train_cat_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# 2000번부터 2499번의 고양이 이미지를 test/cat 폴더로 이동\n",
    "fnames = [\"cat.{}.jpg\".format(i) for i in range(2000, 2500)]\n",
    "for fname in fnames :\n",
    "    src = os.path.join(train_dir2, fname)\n",
    "    dst = os.path.join(test_cat_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# 0번부터 1999번의 개 이미지를 train/dog 폴더로 이동\n",
    "fnames = [\"dog.{}.jpg\".format(i) for i in range(2000)]\n",
    "for fname in fnames :\n",
    "    src = os.path.join(train_dir2, fname)\n",
    "    dst = os.path.join(train_dog_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# 2000번부터 2499번의 개 이미지를 test/dog 폴더로 이동\n",
    "fnames = [\"dog.{}.jpg\".format(i) for i in range(2000, 2500)]\n",
    "for fname in fnames :\n",
    "    src = os.path.join(train_dir2, fname)\n",
    "    dst = os.path.join(test_dog_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "764de29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련(개) 이미지수 : 2000\n",
      "훈련(고양이) 이미지수 : 2000\n",
      "테스트(개) 이미지수 : 500\n",
      "테스트(고양이) 이미지수 : 500\n"
     ]
    }
   ],
   "source": [
    "# 각 폴더의 이미지 수 확인\n",
    "print(\"훈련(개) 이미지수 : {}\".format(len(os.listdir(train_dog_dir))))\n",
    "print(\"훈련(고양이) 이미지수 : {}\".format(len(os.listdir(train_cat_dir))))\n",
    "print(\"테스트(개) 이미지수 : {}\".format(len(os.listdir(test_dog_dir))))\n",
    "print(\"테스트(고양이) 이미지수 : {}\".format(len(os.listdir(test_cat_dir))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a0b0e",
   "metadata": {},
   "source": [
    "## 이미지 전처리\n",
    "\n",
    "- 0-255 범위의 픽셀값을 0.0-1.0 범위로 변경\n",
    "- 이미지 크기를 동일하게 변환\n",
    "- 이미지에 라벨을 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e46d708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# ImageDataGenerator : 반복해서 이미지를 전처리하는 라이브러리\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 정규화 설정 (0-255 -> 0.0-1.0변환)\n",
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 학습용 이미지 전처리 설정\n",
    "train_generator = train_gen.flow_from_directory(\n",
    "    # 해당 폴더\n",
    "    train_dir,\n",
    "    # 이미지 크기를 동일하게 변환\n",
    "    target_size = (150, 150),\n",
    "    batch_size = 20,    # 한 번에 처리하는 이미지의 개수\n",
    "    # binary : 0과 1로 라벨링 (디렉토리명의 알파벳순으로)\n",
    "    # categorical : 0부터의 개수만큼 1씩 증가시켜 라벨링 \n",
    "    #               (디렉토리명의 알파벳순으로)\n",
    "    class_mode = \"binary\"\n",
    ")\n",
    "\n",
    "test_generator = test_gen.flow_from_directory(\n",
    "    # 해당 폴더\n",
    "    test_dir,\n",
    "    # 이미지 크기를 동일하게 변환\n",
    "    target_size = (150, 150),\n",
    "    batch_size = 20,    # 한 번에 처리하는 이미지의 개수\n",
    "    # binary : 0과 1로 라벨링 (디렉토리명의 알파벳순으로)\n",
    "    # categorical : 0부터의 개수만큼 1씩 증가시켜 라벨링 \n",
    "    #               (디렉토리명의 알파벳순으로)\n",
    "    class_mode = \"binary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccc7af5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cat': 0, 'dog': 1}, {'cat': 0, 'dog': 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라벨링 결과 확인\n",
    "train_generator.class_indices, test_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556a270",
   "metadata": {},
   "source": [
    "## 신경망 설계 (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db65acb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 34, 34, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "# 특성 추출기 모델\n",
    "#  - 최종 특성의 크기가 10x10 정도 될때까지 CNN (Conv2D+Pooling) 층을 쌓음\n",
    "#  - 각 층의 필터의 크기를 몇 개씩 설계할 것인가 \n",
    "#    - 인코딩 : 필터의 개숫를 늘려가는 방식\n",
    "#    - 디코딩 : 필터의 개수를 줄여가는 방식\n",
    "model1.add(Conv2D(filters=32, kernel_size=(3, 3), \n",
    "                  input_shape=(150, 150, 3)))\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model1.add(Conv2D(filters=64, kernel_size=(3, 3)))\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model1.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model1.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model1.add(Flatten())\n",
    "\n",
    "# 분류기 모델\n",
    "model1.add(Dense(units=512, activation=\"relu\"))\n",
    "# loss = \"binary_crossentropy\"\n",
    "model1.add(Dense(units=1, activation=\"sigmoid\"))  \n",
    "# loss = \"categorical_crossentropy\"\n",
    "#model1.add(Dense(units=2, activation=\"softmax\")) \n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "330090c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=\"adam\",\n",
    "               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ede92f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 54s 264ms/step - loss: 0.6961 - accuracy: 0.5251 - val_loss: 0.6708 - val_accuracy: 0.6200\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.6619 - accuracy: 0.5935 - val_loss: 0.6402 - val_accuracy: 0.6380\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.6289 - accuracy: 0.6529 - val_loss: 0.6272 - val_accuracy: 0.6410\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.5631 - accuracy: 0.7043 - val_loss: 0.5863 - val_accuracy: 0.6990\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 0.5035 - accuracy: 0.7588 - val_loss: 0.5320 - val_accuracy: 0.7550\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.4380 - accuracy: 0.7953 - val_loss: 0.5518 - val_accuracy: 0.7270\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 46s 232ms/step - loss: 0.3549 - accuracy: 0.8430 - val_loss: 0.5839 - val_accuracy: 0.7190\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.2866 - accuracy: 0.8661 - val_loss: 0.6002 - val_accuracy: 0.7460\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1826 - accuracy: 0.9295 - val_loss: 0.7903 - val_accuracy: 0.7340\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.0955 - accuracy: 0.9604 - val_loss: 0.8631 - val_accuracy: 0.7230\n"
     ]
    }
   ],
   "source": [
    "h1 = model1.fit(train_generator, epochs=10,\n",
    "                validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a06cf7c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결과 시각화 하기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = h1.history[\"accuracy\"]\n",
    "val_acc = h1.history[\"val_accuracy\"]\n",
    "loss = h1.history[\"loss\"]\n",
    "val_loss = h1.history[\"val_loss\"]\n",
    "\n",
    "e = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(e, acc, \"r\", label=\"train acc\")\n",
    "plt.plot(e, val_acc, \"b\", label=\"test acc\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(e, loss, \"r\", label=\"train loss\")\n",
    "plt.plot(e, val_loss, \"b\", label=\"test loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70af85ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\\cat.2000.jpg\n",
      "['고양이']\n",
      "cat\\cat.2001.jpg\n",
      "['개']\n",
      "cat\\cat.2002.jpg\n",
      "['고양이']\n",
      "cat\\cat.2003.jpg\n",
      "['개']\n",
      "cat\\cat.2004.jpg\n",
      "['고양이']\n",
      "cat\\cat.2005.jpg\n",
      "['개']\n",
      "cat\\cat.2006.jpg\n",
      "['개']\n",
      "cat\\cat.2007.jpg\n",
      "['개']\n",
      "cat\\cat.2008.jpg\n",
      "['고양이']\n",
      "cat\\cat.2009.jpg\n",
      "['개']\n"
     ]
    }
   ],
   "source": [
    "# 예측하기\n",
    "import numpy as np\n",
    "\n",
    "pred = model1.predict_generator(test_generator)\n",
    "\n",
    "pred_val = np.where(pred > .5, \"개\", \"고양이\")\n",
    "\n",
    "for i in range(10) :\n",
    "    print(test_generator.filenames[i])\n",
    "    print(pred_val[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9f3b4",
   "metadata": {},
   "source": [
    "## 데이터 확장 (data augmentation, 증식)을 이용한 성능개선\n",
    "\n",
    "- 원본 이미지 데이터에 변형을 주어서 변형된 데이터를 CNN에 입력해주는 방법\n",
    "- 데이터의 수가 증가하는 효과가 있음\n",
    "- 변형 방법 : 회전, 이동, 확대/축소, 뒤집기, 기울림, 밝기조정 등\n",
    "- 증식을 하면 오차가 커지므로 epochs 값을 늘려서 학습을 해주어야 함\n",
    "- 증식은 훈련데이터에만 적용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44948fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 증식 설정 (ImageDataGenerator 활용) \n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range = 20,   #20도 내로 회전\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.1,\n",
    "    shear_range = 0.1,\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    vertical_flip = False,\n",
    "    fill_mode = \"nearest\"    # 이미지 변형시 깨지는 부분을 보상\n",
    ")\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (150, 150),\n",
    "    batch_size = 20,    \n",
    "    class_mode = \"binary\"\n",
    ")\n",
    "\n",
    "test_generator = test_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (150, 150),\n",
    "    batch_size = 20,    \n",
    "    class_mode = \"binary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25d1eb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 34, 34, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "# 특성 추출기 모델\n",
    "#  - 최종 특성의 크기가 10x10 정도 될때까지 CNN (Conv2D+Pooling) 층을 쌓음\n",
    "#  - 각 층의 필터의 크기를 몇 개씩 설계할 것인가 \n",
    "#    - 인코딩 : 필터의 개숫를 늘려가는 방식\n",
    "#    - 디코딩 : 필터의 개수를 줄여가는 방식\n",
    "model2.add(Conv2D(filters=32, kernel_size=(3, 3), \n",
    "                  input_shape=(150, 150, 3)))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model2.add(Conv2D(filters=64, kernel_size=(3, 3)))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model2.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model2.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "# 분류기 모델\n",
    "model2.add(Dense(units=512, activation=\"relu\"))\n",
    "# loss = \"binary_crossentropy\"\n",
    "model2.add(Dense(units=1, activation=\"sigmoid\"))  \n",
    "# loss = \"categorical_crossentropy\"\n",
    "#model1.add(Dense(units=2, activation=\"softmax\")) \n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46c8e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=\"adam\",\n",
    "               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = model2.fit(train_generator, epochs=30,\n",
    "                validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53592c7",
   "metadata": {},
   "source": [
    "## 전이학습 (Transfer Learning)을 이용한 성능 개선\n",
    "\n",
    "- 특성추출 : 기존의 모델에 있는 CNN층을 그대로 가져다가 사용\n",
    "  - 가져온 CNN 층에 대해서 오차역전파를 수행하지 않음 (동결)\n",
    "- 미세조정 : 기존의 모델에 있는 CNN층을 그대로 가져다가 사용\n",
    "  - 가져온 CNN 층에 대해서 분류기와 가까운 층들에 대해 오차역전파를 수행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18fc980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 5s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# VGG16 모델을 가져온다\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# weights : 어떤 가중치를 가져올 것인지 설정\n",
    "# include_top : 분류기도 가져올 것인지 설정\n",
    "# input_shape : 우리 모델에서 사용할 입력 이미지 크기\n",
    "conv_base = VGG16(weights=\"imagenet\",\n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9413e743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 18,910,017\n",
      "Trainable params: 18,910,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model3 = Sequential()\n",
    "\n",
    "# VGG16 모델을 우리 모델에 연결\n",
    "model3.add(conv_base)\n",
    "\n",
    "model3.add(Flatten())\n",
    "\n",
    "# 분류기 모델\n",
    "model3.add(Dense(units=512, activation=\"relu\"))\n",
    "model3.add(Dense(units=1, activation=\"sigmoid\"))  \n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953be57",
   "metadata": {},
   "source": [
    "- 동결 : 가져온 모델로는 오차역전파가 되지 않도록 막는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d74fb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습가능한 층의 수 :  30\n"
     ]
    }
   ],
   "source": [
    "# 학습이 되는 층을 확인\n",
    "# 학습가능한 VGG16모델의 파라미터\n",
    "#      - Conv2 층이 13개이므로 각 w(필터), b가 있어 26개\n",
    "# 학습가능한 우리 모델의 파라미터 : Dense가 2개에 w, b가 있어 총 4개\n",
    "print(\"학습가능한 파라미터 수 : \", len(model3.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bbdeb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동결작업 : VGG16의 파라미터는 학습이 되지 않도록 막음\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f97a3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습가능한 파라미터 수 :  4\n"
     ]
    }
   ],
   "source": [
    "print(\"학습가능한 파라미터 수 : \", len(model3.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d645d9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 119s 594ms/step - loss: 0.5856 - accuracy: 0.7618 - val_loss: 0.2500 - val_accuracy: 0.8980\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 121s 606ms/step - loss: 0.2921 - accuracy: 0.8689 - val_loss: 0.2789 - val_accuracy: 0.8770\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 123s 615ms/step - loss: 0.2761 - accuracy: 0.8747 - val_loss: 0.2472 - val_accuracy: 0.8890\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 123s 617ms/step - loss: 0.2355 - accuracy: 0.9029 - val_loss: 0.2379 - val_accuracy: 0.8950\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 124s 618ms/step - loss: 0.2368 - accuracy: 0.9054 - val_loss: 0.2658 - val_accuracy: 0.8780\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 125s 626ms/step - loss: 0.2207 - accuracy: 0.9072 - val_loss: 0.2956 - val_accuracy: 0.8780\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 121s 605ms/step - loss: 0.2198 - accuracy: 0.9079 - val_loss: 0.2428 - val_accuracy: 0.8960\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 119s 598ms/step - loss: 0.2324 - accuracy: 0.9040 - val_loss: 0.2355 - val_accuracy: 0.9040\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 120s 600ms/step - loss: 0.1968 - accuracy: 0.9191 - val_loss: 0.2938 - val_accuracy: 0.8890\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 119s 596ms/step - loss: 0.1951 - accuracy: 0.9211 - val_loss: 0.2541 - val_accuracy: 0.8920\n"
     ]
    }
   ],
   "source": [
    "model3.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=\"adam\",\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "h3 = model3.fit(train_generator, epochs=10,\n",
    "                validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd31492",
   "metadata": {},
   "source": [
    "### 전이학습 : 미세조정\n",
    "\n",
    "- 분류기에서 가까운 층의 동결을 풀어주어서 학습이 되도록 조정\n",
    "- 미세조정을 하면 우리모델과 가져온 모델 간의 매칭도가 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0fde69b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 18,910,017\n",
      "Trainable params: 11,274,753\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model4 = Sequential()\n",
    "\n",
    "# VGG16 모델을 우리 모델에 연결\n",
    "model4.add(conv_base)\n",
    "\n",
    "model4.add(Flatten())\n",
    "\n",
    "# 분류기 모델\n",
    "model4.add(Dense(units=512, activation=\"relu\"))\n",
    "model4.add(Dense(units=1, activation=\"sigmoid\"))  \n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f06ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16의 전체 층의 동결을 푼다\n",
    "conv_base.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a20824d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16의 5층만 동결 (분류기에서 가장 가까운 층)\n",
    "set_trainable = False  # 동결 여부\n",
    "\n",
    "for layer in conv_base.layers :\n",
    "    if layer.name == \"block5_conv3\" :\n",
    "        set_trainable = True\n",
    "    \n",
    "    if set_trainable :\n",
    "        layer.trainable = True    # 5층의 동결을 푼다\n",
    "    else :\n",
    "        layer.trainable = False   # 1-4층은 동결        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2527772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습가능한 파라미터 수 :  6\n"
     ]
    }
   ],
   "source": [
    "print(\"학습가능한 파라미터 수 : \", len(model4.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb6a2ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 149s 740ms/step - loss: 0.8117 - accuracy: 0.4932 - val_loss: 0.6932 - val_accuracy: 0.4980\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 153s 766ms/step - loss: 0.7502 - accuracy: 0.5030 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      " 74/200 [==========>...................] - ETA: 1:27 - loss: 0.6931 - accuracy: 0.5127"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-bf6f39caccd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m h4 = model4.fit(train_generator, epochs=10,\n\u001b[1;32m----> 6\u001b[1;33m                 validation_data=test_generator)\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model4.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=\"adam\",\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "h4 = model4.fit(train_generator, epochs=10,\n",
    "                validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe9f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb68b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72e46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378b3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc8e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8415a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ccc02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
